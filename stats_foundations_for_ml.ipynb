{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Foundations for Machine Learning\n",
    "\n",
    "* * * \n",
    "\n",
    "### Icons used in this notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
    "‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what's possible!<br>\n",
    "üìù **Notation**: Breaking down mathematical notation into plain English.<br>\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "\n",
    "1. [Describe data using summary statistics](#part1) ‚Äì mean, variance, and standard deviation\n",
    "2. [Understand distributions](#part2) ‚Äì especially the normal distribution and why it matters\n",
    "3. [Reason from samples to populations](#part3) ‚Äì the sampling distribution and standard error\n",
    "4. [Quantify uncertainty](#part4) ‚Äì confidence intervals and what they really mean\n",
    "5. [Evaluate evidence](#part5) ‚Äì hypothesis testing, p-values, and effect sizes\n",
    "6. [Understand relationships](#part6) ‚Äì correlation and its limitations\n",
    "7. [Bridge to ML/NLP](#part7) ‚Äì vectors, matrices, and similarity measures\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This workshop assumes basic Python familiarity (variables, lists, functions). No prior statistics knowledge is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the libraries we'll use throughout this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nicer plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Dataset: Community Survey\n",
    "\n",
    "Throughout this workshop, we'll work with a simulated dataset from a community survey. This dataset contains information about 200 respondents, including their demographics and attitudes.\n",
    "\n",
    "Let's create and explore this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our simulated survey dataset\n",
    "n = 200\n",
    "\n",
    "# Generate correlated variables\n",
    "education_years = np.random.normal(14, 3, n).clip(8, 22).round()\n",
    "age = np.random.normal(42, 15, n).clip(18, 85).round()\n",
    "\n",
    "# Income correlates with education (with noise)\n",
    "income = (25000 + education_years * 4000 + np.random.normal(0, 15000, n)).clip(15000, 200000).round(-2)\n",
    "\n",
    "# Political engagement correlates with education and age\n",
    "engagement_score = (\n",
    "    2 + \n",
    "    0.3 * (education_years - 14) + \n",
    "    0.02 * (age - 42) + \n",
    "    np.random.normal(0, 1.5, n)\n",
    ").clip(0, 10).round(1)\n",
    "\n",
    "# Social trust score\n",
    "social_trust = (\n",
    "    5 + \n",
    "    0.2 * (education_years - 14) + \n",
    "    np.random.normal(0, 2, n)\n",
    ").clip(0, 10).round(1)\n",
    "\n",
    "# News consumption (hours per week)\n",
    "news_hours = np.random.exponential(5, n).clip(0, 30).round(1)\n",
    "\n",
    "# Create DataFrame\n",
    "survey = pd.DataFrame({\n",
    "    'age': age,\n",
    "    'education_years': education_years,\n",
    "    'income': income,\n",
    "    'social_trust': social_trust,\n",
    "    'news_hours': news_hours,\n",
    "    'political_engagement': engagement_score\n",
    "})\n",
    "\n",
    "survey.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about our dataset\n",
    "print(f\"Dataset shape: {survey.shape[0]} respondents, {survey.shape[1]} variables\")\n",
    "print(f\"\\nVariables:\")\n",
    "for col in survey.columns:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part1'></a>\n",
    "\n",
    "# Part 1: Describing Data\n",
    "\n",
    "Before we can make claims about the world, we need to describe what we see in our data. This section covers the fundamental tools for summarizing data.\n",
    "\n",
    "## The Big Question\n",
    "\n",
    "> If someone asked you to describe a variable in your dataset using just one or two numbers, what would you tell them?\n",
    "\n",
    "## Central Tendency: Where is the \"Middle\"?\n",
    "\n",
    "The most basic question about a variable is: **What's a typical value?**\n",
    "\n",
    "We have three main ways to answer this:\n",
    "\n",
    "| Measure | What it is | When to use it |\n",
    "|---------|------------|----------------|\n",
    "| **Mean** | Sum of all values √∑ count | Symmetric data without extreme outliers |\n",
    "| **Median** | Middle value when sorted | Skewed data or when outliers are present |\n",
    "| **Mode** | Most frequent value | Categorical data or finding peaks |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mean (Average)\n",
    "\n",
    "The mean is the \"center of gravity\" of your data. Add up all values and divide by how many you have.\n",
    "\n",
    "üìù **Notation**: \n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i$$\n",
    "\n",
    "Let's break this down piece by piece:\n",
    "- $\\bar{x}$ (read as \"x-bar\") is the mean\n",
    "- $n$ is how many data points we have\n",
    "- $\\sum$ (capital sigma) means \"add up\"\n",
    "- $x_i$ means \"the i-th value\" (so $x_1$ is the first value, $x_2$ is the second, etc.)\n",
    "- $\\sum_{i=1}^{n}$ means \"add up from i=1 to i=n\" ‚Äî in other words, add up all values\n",
    "\n",
    "In plain English: **Add up all the x values, then divide by how many there are.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate the mean of age, step by step\n",
    "ages = survey['age'].values\n",
    "\n",
    "# Step 1: Sum all values\n",
    "total = sum(ages)\n",
    "print(f\"Sum of all ages: {total}\")\n",
    "\n",
    "# Step 2: Count how many values\n",
    "n = len(ages)\n",
    "print(f\"Number of respondents: {n}\")\n",
    "\n",
    "# Step 3: Divide\n",
    "mean_age = total / n\n",
    "print(f\"Mean age: {mean_age:.1f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course, NumPy and pandas do this for us\n",
    "print(f\"Mean age (numpy): {np.mean(ages):.1f}\")\n",
    "print(f\"Mean age (pandas): {survey['age'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Median\n",
    "\n",
    "The median is the middle value when you sort your data. Half of the values are above it, half below.\n",
    "\n",
    "Why do we need both mean and median? Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mean and median for income\n",
    "print(f\"Income - Mean: ${survey['income'].mean():,.0f}\")\n",
    "print(f\"Income - Median: ${survey['income'].median():,.0f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Age (roughly symmetric)\n",
    "axes[0].hist(survey['age'], bins=20, edgecolor='white', alpha=0.7)\n",
    "axes[0].axvline(survey['age'].mean(), color='red', linestyle='--', label=f\"Mean: {survey['age'].mean():.1f}\")\n",
    "axes[0].axvline(survey['age'].median(), color='blue', linestyle='--', label=f\"Median: {survey['age'].median():.1f}\")\n",
    "axes[0].set_xlabel('Age')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Age Distribution (roughly symmetric)')\n",
    "axes[0].legend()\n",
    "\n",
    "# News hours (skewed)\n",
    "axes[1].hist(survey['news_hours'], bins=20, edgecolor='white', alpha=0.7)\n",
    "axes[1].axvline(survey['news_hours'].mean(), color='red', linestyle='--', label=f\"Mean: {survey['news_hours'].mean():.1f}\")\n",
    "axes[1].axvline(survey['news_hours'].median(), color='blue', linestyle='--', label=f\"Median: {survey['news_hours'].median():.1f}\")\n",
    "axes[1].set_xlabel('News Consumption (hours/week)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('News Hours (right-skewed)')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: Look at the two histograms above. In which case are the mean and median similar? In which case are they different? Why?\n",
    "\n",
    "üí° **Tip**: When data is skewed (has a long tail in one direction), the mean gets \"pulled\" toward the tail. The median is more robust to outliers and skew."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spread: How Variable is the Data?\n",
    "\n",
    "Knowing the center isn't enough. Two datasets can have the same mean but look completely different:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two datasets with the same mean, different spread\n",
    "narrow = np.random.normal(50, 5, 1000)   # Mean 50, small spread\n",
    "wide = np.random.normal(50, 20, 1000)    # Mean 50, large spread\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(narrow, bins=30, edgecolor='white', alpha=0.7)\n",
    "axes[0].axvline(np.mean(narrow), color='red', linestyle='--')\n",
    "axes[0].set_xlim(-20, 120)\n",
    "axes[0].set_title(f'Narrow spread (mean = {np.mean(narrow):.1f})')\n",
    "\n",
    "axes[1].hist(wide, bins=30, edgecolor='white', alpha=0.7)\n",
    "axes[1].axvline(np.mean(wide), color='red', linestyle='--')\n",
    "axes[1].set_xlim(-20, 120)\n",
    "axes[1].set_title(f'Wide spread (mean = {np.mean(wide):.1f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Both have nearly the same mean, but very different spreads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance and Standard Deviation\n",
    "\n",
    "We need a number that captures \"how spread out\" the data is. Here's the idea:\n",
    "\n",
    "1. Find how far each point is from the mean\n",
    "2. Square these distances (to make them all positive)\n",
    "3. Take the average of these squared distances ‚Üí **Variance**\n",
    "4. Take the square root to get back to original units ‚Üí **Standard Deviation**\n",
    "\n",
    "üìù **Notation**:\n",
    "\n",
    "**Variance** (sigma squared):\n",
    "$$\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2$$\n",
    "\n",
    "**Standard Deviation** (sigma):\n",
    "$$\\sigma = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(x_i - \\bar{x})^2}$$\n",
    "\n",
    "Let's break this down:\n",
    "- $(x_i - \\bar{x})$ is how far point $i$ is from the mean (the \"deviation\")\n",
    "- $(x_i - \\bar{x})^2$ is that deviation squared\n",
    "- We sum all squared deviations and divide by $n$ to get the average\n",
    "\n",
    "In plain English: **The variance is the average squared distance from the mean. The standard deviation is the square root of that.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance step by step for education_years\n",
    "edu = survey['education_years'].values\n",
    "\n",
    "# Step 1: Calculate the mean\n",
    "mean_edu = np.mean(edu)\n",
    "print(f\"Mean education: {mean_edu:.2f} years\")\n",
    "\n",
    "# Step 2: Calculate deviations from the mean\n",
    "deviations = edu - mean_edu\n",
    "print(f\"\\nFirst 5 deviations: {deviations[:5].round(2)}\")\n",
    "print(f\"(These show how far each person is from the mean)\")\n",
    "\n",
    "# Step 3: Square the deviations\n",
    "squared_deviations = deviations ** 2\n",
    "print(f\"\\nFirst 5 squared deviations: {squared_deviations[:5].round(2)}\")\n",
    "\n",
    "# Step 4: Take the mean of squared deviations = Variance\n",
    "variance = np.mean(squared_deviations)\n",
    "print(f\"\\nVariance: {variance:.2f} years¬≤\")\n",
    "\n",
    "# Step 5: Square root = Standard Deviation\n",
    "std_dev = np.sqrt(variance)\n",
    "print(f\"Standard Deviation: {std_dev:.2f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with NumPy\n",
    "print(f\"NumPy std: {np.std(edu):.2f}\")\n",
    "print(f\"Our calculation: {std_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning**: You may see formulas that divide by $(n-1)$ instead of $n$. This is the \"sample standard deviation\" and is used when estimating the population SD from a sample. For now, don't worry about this distinction ‚Äî both give very similar results for large samples. NumPy uses $n$ by default; pandas uses $n-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Standard Deviation\n",
    "\n",
    "The standard deviation tells you: **On average, how far are data points from the mean?**\n",
    "\n",
    "- Small SD ‚Üí data points cluster tightly around the mean\n",
    "- Large SD ‚Üí data points are spread out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for all variables\n",
    "summary_stats = survey.describe().T[['mean', 'std', 'min', 'max']]\n",
    "summary_stats.columns = ['Mean', 'Std Dev', 'Min', 'Max']\n",
    "summary_stats.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 1: Calculate Summary Statistics\n",
    "\n",
    "Calculate the mean, median, and standard deviation for the `social_trust` variable **by hand** (using basic Python operations), then verify with NumPy/pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the social_trust values\n",
    "trust = survey['social_trust'].values\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Calculate mean\n",
    "mean_trust = ___\n",
    "\n",
    "# Calculate median (hint: sort first, then find middle value)\n",
    "median_trust = ___\n",
    "\n",
    "# Calculate standard deviation\n",
    "std_trust = ___\n",
    "\n",
    "print(f\"Mean: {mean_trust:.2f}\")\n",
    "print(f\"Median: {median_trust:.2f}\")\n",
    "print(f\"Std Dev: {std_trust:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify with NumPy\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Mean: {np.mean(trust):.2f}\")\n",
    "print(f\"Median: {np.median(trust):.2f}\")\n",
    "print(f\"Std Dev: {np.std(trust):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part2'></a>\n",
    "\n",
    "# Part 2: The Shape of Data\n",
    "\n",
    "Data has shape, and that shape matters. The most important shape in statistics is the **normal distribution** (also called the bell curve or Gaussian distribution).\n",
    "\n",
    "## The Normal Distribution\n",
    "\n",
    "The normal distribution appears everywhere:\n",
    "- Heights and weights of people\n",
    "- Measurement errors\n",
    "- Test scores\n",
    "- Many natural phenomena\n",
    "\n",
    "It's defined by just two parameters:\n",
    "- **Œº (mu)**: the mean (center of the bell)\n",
    "- **œÉ (sigma)**: the standard deviation (width of the bell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize normal distributions with different parameters\n",
    "x = np.linspace(-10, 20, 1000)\n",
    "\n",
    "def normal_pdf(x, mu, sigma):\n",
    "    \"\"\"Calculate the normal probability density function.\"\"\"\n",
    "    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Different means\n",
    "for mu in [0, 5, 10]:\n",
    "    axes[0].plot(x, normal_pdf(x, mu, 2), label=f'Œº = {mu}')\n",
    "axes[0].set_title('Same spread (œÉ = 2), different centers')\n",
    "axes[0].set_xlabel('Value')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].legend()\n",
    "\n",
    "# Different standard deviations\n",
    "for sigma in [1, 2, 4]:\n",
    "    axes[1].plot(x, normal_pdf(x, 5, sigma), label=f'œÉ = {sigma}')\n",
    "axes[1].set_title('Same center (Œº = 5), different spreads')\n",
    "axes[1].set_xlabel('Value')\n",
    "axes[1].set_ylabel('Density')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 68-95-99.7 Rule\n",
    "\n",
    "For normally distributed data, there's a handy rule:\n",
    "\n",
    "- **68%** of data falls within **1 standard deviation** of the mean\n",
    "- **95%** of data falls within **2 standard deviations** of the mean\n",
    "- **99.7%** of data falls within **3 standard deviations** of the mean\n",
    "\n",
    "This is incredibly useful for understanding what values are \"typical\" vs. \"unusual.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 68-95-99.7 rule\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = normal_pdf(x, 0, 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(x, y, 'k', linewidth=2)\n",
    "\n",
    "# Fill regions\n",
    "ax.fill_between(x, y, where=(x >= -1) & (x <= 1), alpha=0.3, color='blue', label='68% (¬±1œÉ)')\n",
    "ax.fill_between(x, y, where=((x >= -2) & (x < -1)) | ((x > 1) & (x <= 2)), alpha=0.3, color='green', label='95% (¬±2œÉ)')\n",
    "ax.fill_between(x, y, where=((x >= -3) & (x < -2)) | ((x > 2) & (x <= 3)), alpha=0.3, color='orange', label='99.7% (¬±3œÉ)')\n",
    "\n",
    "ax.set_xlabel('Standard Deviations from Mean', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('The 68-95-99.7 Rule', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xlim(-4, 4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Scores: Standardizing Data\n",
    "\n",
    "A **z-score** tells you how many standard deviations a value is from the mean.\n",
    "\n",
    "üìù **Notation**:\n",
    "\n",
    "$$z = \\frac{x - \\bar{x}}{\\sigma}$$\n",
    "\n",
    "In plain English: **Subtract the mean, then divide by the standard deviation.**\n",
    "\n",
    "After this transformation:\n",
    "- A z-score of 0 means the value equals the mean\n",
    "- A z-score of 1 means the value is one SD above the mean\n",
    "- A z-score of -2 means the value is two SDs below the mean\n",
    "\n",
    "Z-scores let you compare values from different distributions and are essential for data preprocessing in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate z-scores for age\n",
    "age_mean = survey['age'].mean()\n",
    "age_std = survey['age'].std()\n",
    "\n",
    "survey['age_zscore'] = (survey['age'] - age_mean) / age_std\n",
    "\n",
    "# Look at a few examples\n",
    "print(\"Original ages and their z-scores:\")\n",
    "print(survey[['age', 'age_zscore']].head(10).to_string())\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"Mean age: {age_mean:.1f}, SD: {age_std:.1f}\")\n",
    "print(f\"A z-score of 1.0 means the person is {age_std:.1f} years older than average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After standardization, the z-scores have mean ‚âà 0 and SD ‚âà 1\n",
    "print(f\"Z-score mean: {survey['age_zscore'].mean():.6f}\")\n",
    "print(f\"Z-score std: {survey['age_zscore'].std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: If someone has an income z-score of 2.5, what can you say about their income relative to the sample?\n",
    "\n",
    "üí° **Tip**: In ML preprocessing, we often standardize all features so they're on the same scale. This prevents features with large values (like income) from dominating features with small values (like age)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Data Isn't Normal\n",
    "\n",
    "Not all data follows a normal distribution. It's important to visualize your data before assuming normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions of different variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "variables = ['age', 'education_years', 'income', 'news_hours']\n",
    "titles = ['Age (roughly normal)', 'Education (bounded)', 'Income (right-skewed)', 'News Hours (exponential)']\n",
    "\n",
    "for ax, var, title in zip(axes.flat, variables, titles):\n",
    "    ax.hist(survey[var], bins=25, edgecolor='white', alpha=0.7, density=True)\n",
    "    ax.set_xlabel(var)\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 2: Find Unusual Values\n",
    "\n",
    "Using z-scores, find respondents with \"unusual\" income (more than 2 standard deviations from the mean). How many are there? Are they unusually high, low, or both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# Step 1: Calculate z-scores for income\n",
    "income_zscore = ___\n",
    "\n",
    "# Step 2: Find respondents with |z| > 2\n",
    "unusual = ___\n",
    "\n",
    "# Step 3: Examine them\n",
    "print(f\"Number of unusual income values: ___\")\n",
    "print(f\"\\nThese respondents:\")\n",
    "# Show the unusual cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "# Part 3: From Sample to Population\n",
    "\n",
    "This is the heart of statistics. We almost never see the whole population ‚Äî we only see a sample. How can we make claims about the population based on our sample?\n",
    "\n",
    "## The Problem\n",
    "\n",
    "Imagine we want to know the average political engagement of all adults in California. We can't survey everyone, so we survey 200 people.\n",
    "\n",
    "Our sample mean is 4.8. But:\n",
    "- If we surveyed a *different* 200 people, would we get exactly 4.8 again?\n",
    "- Probably not! We'd get something close, but not identical.\n",
    "\n",
    "This is **sampling variability**: different samples give different results.\n",
    "\n",
    "## The Sampling Distribution\n",
    "\n",
    "Here's a thought experiment:\n",
    "1. Take a sample of 200 people, calculate the mean\n",
    "2. Repeat this 1000 times\n",
    "3. Plot all 1000 means\n",
    "\n",
    "The distribution of these sample means is called the **sampling distribution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the sampling distribution\n",
    "# Pretend our survey IS the population (for demonstration)\n",
    "\n",
    "# We'll repeatedly sample from a larger \"population\"\n",
    "population = np.random.normal(50, 15, 100000)  # Large population, mean=50, sd=15\n",
    "\n",
    "# Take many samples of different sizes and compute their means\n",
    "sample_sizes = [10, 30, 100]\n",
    "n_samples = 1000\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, n in zip(axes, sample_sizes):\n",
    "    sample_means = [np.mean(np.random.choice(population, n, replace=False)) \n",
    "                    for _ in range(n_samples)]\n",
    "    \n",
    "    ax.hist(sample_means, bins=40, edgecolor='white', alpha=0.7, density=True)\n",
    "    ax.axvline(50, color='red', linestyle='--', label=f'True mean = 50')\n",
    "    ax.axvline(np.mean(sample_means), color='blue', linestyle='--', \n",
    "               label=f'Mean of sample means = {np.mean(sample_means):.2f}')\n",
    "    ax.set_xlabel('Sample Mean')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Sample size n = {n}\\nSD of sample means = {np.std(sample_means):.2f}')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_xlim(35, 65)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: Look at the three plots above. As sample size increases, what happens to the spread of the sampling distribution? Why does this matter?\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "Two crucial observations from the simulation:\n",
    "\n",
    "1. **The sampling distribution is centered on the true population mean.** Sample means aren't biased ‚Äî on average, they equal the population mean.\n",
    "\n",
    "2. **Larger samples give more precise estimates.** The spread of the sampling distribution shrinks as sample size grows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Central Limit Theorem\n",
    "\n",
    "One of the most important results in statistics:\n",
    "\n",
    "> **The sampling distribution of the mean is approximately normal, regardless of the shape of the population distribution, as long as the sample size is large enough.**\n",
    "\n",
    "This is why the normal distribution is so central to statistics ‚Äî it describes how sample means behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the Central Limit Theorem with a non-normal population\n",
    "# Let's use an exponential distribution (very skewed)\n",
    "\n",
    "skewed_population = np.random.exponential(10, 100000)  # Very right-skewed\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Top row: the population\n",
    "axes[0, 0].hist(skewed_population, bins=50, edgecolor='white', alpha=0.7, density=True)\n",
    "axes[0, 0].set_title('Population Distribution (exponential)')\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "\n",
    "axes[0, 1].axis('off')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Bottom row: sampling distributions for different sample sizes\n",
    "sample_sizes = [5, 30, 100]\n",
    "\n",
    "for ax, n in zip(axes[1], sample_sizes):\n",
    "    sample_means = [np.mean(np.random.choice(skewed_population, n, replace=False)) \n",
    "                    for _ in range(1000)]\n",
    "    \n",
    "    ax.hist(sample_means, bins=40, edgecolor='white', alpha=0.7, density=True)\n",
    "    ax.set_xlabel('Sample Mean')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'Sampling Distribution (n = {n})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Even though the population is very skewed,\")\n",
    "print(\"the sampling distribution of the mean becomes normal as n increases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Error: The Precision of Our Estimate\n",
    "\n",
    "The **standard error (SE)** is the standard deviation of the sampling distribution. It tells us how much sample means typically vary from the true population mean.\n",
    "\n",
    "üìù **Notation**:\n",
    "\n",
    "$$SE = \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "Where:\n",
    "- $\\sigma$ is the population standard deviation\n",
    "- $n$ is the sample size\n",
    "\n",
    "In plain English: **The standard error equals the SD divided by the square root of the sample size.**\n",
    "\n",
    "Key insights:\n",
    "- Larger sample ‚Üí smaller SE ‚Üí more precise estimate\n",
    "- To halve the SE, you need to quadruple the sample size (because of the square root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard error for our survey\n",
    "engagement = survey['political_engagement']\n",
    "\n",
    "sample_mean = engagement.mean()\n",
    "sample_std = engagement.std()\n",
    "n = len(engagement)\n",
    "\n",
    "# Standard Error\n",
    "SE = sample_std / np.sqrt(n)\n",
    "\n",
    "print(f\"Sample mean: {sample_mean:.3f}\")\n",
    "print(f\"Sample SD: {sample_std:.3f}\")\n",
    "print(f\"Sample size: {n}\")\n",
    "print(f\"Standard Error: {SE:.3f}\")\n",
    "print(f\"\\nInterpretation: Our sample mean is likely within about {SE:.3f} points\")\n",
    "print(f\"of the true population mean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning**: Don't confuse **standard deviation** (spread of individual data points) with **standard error** (precision of the mean estimate). They answer different questions:\n",
    "- SD: \"How spread out are people's engagement scores?\"\n",
    "- SE: \"How precisely do we know the average engagement score?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 3: Sample Size and Precision\n",
    "\n",
    "Calculate the standard error for the mean income:\n",
    "1. Using all 200 respondents\n",
    "2. Using only the first 50 respondents\n",
    "3. Using only the first 25 respondents\n",
    "\n",
    "What happens to the SE as sample size decreases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "income = survey['income'].values\n",
    "\n",
    "# Calculate SE for n=200\n",
    "se_200 = ___\n",
    "\n",
    "# Calculate SE for n=50\n",
    "se_50 = ___\n",
    "\n",
    "# Calculate SE for n=25\n",
    "se_25 = ___\n",
    "\n",
    "print(f\"SE with n=200: ${se_200:,.0f}\")\n",
    "print(f\"SE with n=50: ${se_50:,.0f}\")\n",
    "print(f\"SE with n=25: ${se_25:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "# Part 4: Quantifying Uncertainty\n",
    "\n",
    "We have a sample mean, and we know it's probably close to the population mean (thanks to the Central Limit Theorem). But how close? We need to quantify our uncertainty.\n",
    "\n",
    "## Confidence Intervals\n",
    "\n",
    "A **confidence interval** gives us a range of plausible values for the population parameter.\n",
    "\n",
    "üìù **Notation** (for a 95% confidence interval):\n",
    "\n",
    "$$CI = \\bar{x} \\pm 1.96 \\times SE$$\n",
    "\n",
    "Where:\n",
    "- $\\bar{x}$ is the sample mean\n",
    "- 1.96 is the z-value that captures 95% of a normal distribution\n",
    "- $SE$ is the standard error\n",
    "\n",
    "In plain English: **Take the sample mean, then go about 2 standard errors in each direction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a 95% confidence interval for political engagement\n",
    "engagement = survey['political_engagement']\n",
    "\n",
    "mean = engagement.mean()\n",
    "se = engagement.std() / np.sqrt(len(engagement))\n",
    "\n",
    "# 95% CI\n",
    "ci_lower = mean - 1.96 * se\n",
    "ci_upper = mean + 1.96 * se\n",
    "\n",
    "print(f\"Sample mean: {mean:.3f}\")\n",
    "print(f\"Standard error: {se:.3f}\")\n",
    "print(f\"\\n95% Confidence Interval: [{ci_lower:.3f}, {ci_upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Does \"95% Confident\" Mean?\n",
    "\n",
    "This is often misunderstood!\n",
    "\n",
    "**Wrong interpretation**: \"There's a 95% chance the true mean is in this interval.\"\n",
    "\n",
    "**Correct interpretation**: \"If we repeated this sampling process many times, 95% of the confidence intervals we construct would contain the true mean.\"\n",
    "\n",
    "The true mean is either in the interval or it's not ‚Äî we just don't know which. The \"95%\" refers to the procedure's reliability, not the probability for any single interval.\n",
    "\n",
    "Let's visualize this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate many confidence intervals\n",
    "true_mean = 50\n",
    "true_sd = 15\n",
    "n = 100\n",
    "n_simulations = 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 12))\n",
    "\n",
    "captured = 0\n",
    "for i in range(n_simulations):\n",
    "    sample = np.random.normal(true_mean, true_sd, n)\n",
    "    sample_mean = np.mean(sample)\n",
    "    sample_se = np.std(sample) / np.sqrt(n)\n",
    "    ci_low = sample_mean - 1.96 * sample_se\n",
    "    ci_high = sample_mean + 1.96 * sample_se\n",
    "    \n",
    "    # Check if CI contains true mean\n",
    "    contains_mean = ci_low <= true_mean <= ci_high\n",
    "    color = 'blue' if contains_mean else 'red'\n",
    "    if contains_mean:\n",
    "        captured += 1\n",
    "    \n",
    "    ax.plot([ci_low, ci_high], [i, i], color=color, linewidth=1)\n",
    "    ax.plot(sample_mean, i, 'o', color=color, markersize=3)\n",
    "\n",
    "ax.axvline(true_mean, color='green', linestyle='--', linewidth=2, label=f'True mean = {true_mean}')\n",
    "ax.set_xlabel('Value', fontsize=12)\n",
    "ax.set_ylabel('Sample Number', fontsize=12)\n",
    "ax.set_title(f'100 Confidence Intervals: {captured}% captured the true mean\\n(Blue = captured, Red = missed)', fontsize=12)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting CI Width\n",
    "\n",
    "- **Narrow CI** ‚Üí we have a precise estimate\n",
    "- **Wide CI** ‚Üí lots of uncertainty\n",
    "\n",
    "What makes CIs wider or narrower?\n",
    "- More variability in the data ‚Üí wider CI\n",
    "- Smaller sample size ‚Üí wider CI\n",
    "- Higher confidence level (99% vs 95%) ‚Üí wider CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CIs for all variables\n",
    "def calculate_ci(data, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval for the mean.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = np.std(data) / np.sqrt(n)\n",
    "    \n",
    "    # Z-value for common confidence levels\n",
    "    z_values = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "    z = z_values.get(confidence, 1.96)\n",
    "    \n",
    "    margin = z * se\n",
    "    return mean, mean - margin, mean + margin\n",
    "\n",
    "print(\"95% Confidence Intervals for each variable:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for col in ['age', 'education_years', 'income', 'social_trust', 'political_engagement']:\n",
    "    mean, ci_low, ci_high = calculate_ci(survey[col])\n",
    "    print(f\"{col:25} {mean:10.2f}  [{ci_low:10.2f}, {ci_high:10.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: When you report statistics in research, always include a measure of uncertainty. A mean without a confidence interval or standard error is incomplete information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 4: Compare Confidence Intervals\n",
    "\n",
    "Calculate and compare:\n",
    "1. A 90% confidence interval for mean income\n",
    "2. A 95% confidence interval for mean income  \n",
    "3. A 99% confidence interval for mean income\n",
    "\n",
    "Which is widest? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "income = survey['income']\n",
    "\n",
    "# Hint: Z-values are approximately:\n",
    "# 90% CI: z = 1.645\n",
    "# 95% CI: z = 1.96\n",
    "# 99% CI: z = 2.576\n",
    "\n",
    "# Calculate all three CIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part5'></a>\n",
    "\n",
    "# Part 5: Is This Real or Just Noise?\n",
    "\n",
    "We see patterns in our data all the time. But could they just be random chance? This is where hypothesis testing comes in.\n",
    "\n",
    "## The Logic of Hypothesis Testing\n",
    "\n",
    "Imagine someone claims a coin is fair (50% heads). You flip it 100 times and get 60 heads. Is the coin unfair, or did you just get lucky?\n",
    "\n",
    "The logic of hypothesis testing:\n",
    "\n",
    "1. **Start with skepticism**: Assume there's no effect (the \"null hypothesis\")\n",
    "2. **Ask**: How surprising is our data if the null were true?\n",
    "3. **Decide**: If very surprising, maybe the null is wrong\n",
    "\n",
    "## The Null Hypothesis\n",
    "\n",
    "The **null hypothesis (H‚ÇÄ)** represents the \"boring\" explanation:\n",
    "- \"There is no difference between groups\"\n",
    "- \"There is no relationship between variables\"\n",
    "- \"The true mean equals some specific value\"\n",
    "\n",
    "The **alternative hypothesis (H‚ÇÅ or H‚Çê)** is what we're testing for:\n",
    "- \"There IS a difference\"\n",
    "- \"There IS a relationship\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The P-Value\n",
    "\n",
    "The **p-value** is the probability of seeing data as extreme as ours (or more extreme) **if the null hypothesis were true**.\n",
    "\n",
    "üìù **Important**: The p-value is NOT:\n",
    "- ‚ùå The probability that the null hypothesis is true\n",
    "- ‚ùå The probability that your result is a fluke\n",
    "- ‚ùå The probability that you made an error\n",
    "\n",
    "The p-value IS:\n",
    "- ‚úÖ The probability of the data given the null hypothesis\n",
    "\n",
    "Let's see this with the coin example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coin flip example\n",
    "from scipy import stats\n",
    "\n",
    "# We got 60 heads out of 100 flips\n",
    "n_flips = 100\n",
    "n_heads = 60\n",
    "\n",
    "# If the coin is fair (p=0.5), what's the probability of getting 60+ heads?\n",
    "# This is a one-tailed test; for two-tailed, we'd also consider 40 or fewer\n",
    "\n",
    "# Calculate p-value (two-tailed)\n",
    "p_value = 2 * (1 - stats.binom.cdf(n_heads - 1, n_flips, 0.5))\n",
    "\n",
    "print(f\"Observed: {n_heads} heads out of {n_flips} flips\")\n",
    "print(f\"Expected if fair: {n_flips * 0.5} heads\")\n",
    "print(f\"\\nP-value: {p_value:.4f}\")\n",
    "print(f\"\\nInterpretation: If the coin were fair, there's about a {p_value*100:.1f}% chance\")\n",
    "print(f\"of seeing results this extreme (60+ or 40- heads).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "x = np.arange(0, 101)\n",
    "prob = stats.binom.pmf(x, n_flips, 0.5)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Plot all probabilities\n",
    "ax.bar(x, prob, color='lightblue', edgecolor='white')\n",
    "\n",
    "# Highlight extreme values\n",
    "ax.bar(x[x >= 60], prob[x >= 60], color='red', edgecolor='white', label='60+ heads')\n",
    "ax.bar(x[x <= 40], prob[x <= 40], color='red', edgecolor='white', label='40- heads')\n",
    "\n",
    "ax.axvline(60, color='darkred', linestyle='--', label='Our result (60)')\n",
    "ax.axvline(50, color='green', linestyle='--', label='Expected if fair (50)')\n",
    "\n",
    "ax.set_xlabel('Number of Heads', fontsize=12)\n",
    "ax.set_ylabel('Probability', fontsize=12)\n",
    "ax.set_title('Distribution of Heads in 100 Flips (if coin is fair)\\nRed regions = \"extreme\" results', fontsize=12)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance\n",
    "\n",
    "By convention, we often use **p < 0.05** as a threshold for \"statistical significance.\"\n",
    "\n",
    "- p < 0.05 ‚Üí \"Statistically significant\" ‚Üí We reject the null hypothesis\n",
    "- p ‚â• 0.05 ‚Üí \"Not statistically significant\" ‚Üí We fail to reject the null\n",
    "\n",
    "‚ö†Ô∏è **Warning**: This threshold is arbitrary! There's nothing magical about 0.05. A p-value of 0.049 is not meaningfully different from 0.051. Always consider the context and effect size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Testing a Mean\n",
    "\n",
    "Let's test whether the average political engagement in our sample differs from a hypothetical population value of 5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-sample t-test\n",
    "engagement = survey['political_engagement']\n",
    "hypothesized_mean = 5.0\n",
    "\n",
    "# Perform the test\n",
    "t_stat, p_value = stats.ttest_1samp(engagement, hypothesized_mean)\n",
    "\n",
    "print(f\"Sample mean: {engagement.mean():.3f}\")\n",
    "print(f\"Hypothesized population mean: {hypothesized_mean}\")\n",
    "print(f\"\\nT-statistic: {t_stat:.3f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"\\nConclusion: The difference is statistically significant (p < 0.05).\")\n",
    "    print(f\"We reject the null hypothesis that the true mean equals {hypothesized_mean}.\")\n",
    "else:\n",
    "    print(f\"\\nConclusion: The difference is not statistically significant (p ‚â• 0.05).\")\n",
    "    print(f\"We cannot reject the null hypothesis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect Size: What P-Values Don't Tell You\n",
    "\n",
    "A p-value tells you whether an effect is likely to be \"real\" (not just noise). It does NOT tell you whether the effect is **large enough to matter**.\n",
    "\n",
    "With a large enough sample, even tiny effects become \"statistically significant.\"\n",
    "\n",
    "Always ask:\n",
    "1. Is it statistically significant? (p-value)\n",
    "2. Is it practically significant? (effect size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate how sample size affects significance\n",
    "# True effect is tiny: population mean is 5.1, not 5.0\n",
    "\n",
    "true_mean = 5.1\n",
    "hypothesized = 5.0\n",
    "population_sd = 1.5\n",
    "\n",
    "print(\"Testing whether mean differs from 5.0 when true mean is 5.1\")\n",
    "print(\"(A difference of just 0.1 points!)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for n in [50, 200, 1000, 10000]:\n",
    "    sample = np.random.normal(true_mean, population_sd, n)\n",
    "    t_stat, p_val = stats.ttest_1samp(sample, hypothesized)\n",
    "    print(f\"n = {n:5}: sample mean = {sample.mean():.3f}, p = {p_val:.4f} {'*' if p_val < 0.05 else ''}\")\n",
    "\n",
    "print(\"\\n* = statistically significant at p < 0.05\")\n",
    "print(\"\\nNotice: The same tiny effect becomes 'significant' with enough data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's d: A Common Effect Size Measure\n",
    "\n",
    "**Cohen's d** expresses the difference in terms of standard deviations:\n",
    "\n",
    "$$d = \\frac{\\bar{x} - \\mu_0}{\\sigma}$$\n",
    "\n",
    "Rough guidelines:\n",
    "- d ‚âà 0.2: small effect\n",
    "- d ‚âà 0.5: medium effect\n",
    "- d ‚âà 0.8: large effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cohen's d for our engagement test\n",
    "engagement = survey['political_engagement']\n",
    "hypothesized_mean = 5.0\n",
    "\n",
    "cohens_d = (engagement.mean() - hypothesized_mean) / engagement.std()\n",
    "\n",
    "print(f\"Sample mean: {engagement.mean():.3f}\")\n",
    "print(f\"Hypothesized mean: {hypothesized_mean}\")\n",
    "print(f\"Sample SD: {engagement.std():.3f}\")\n",
    "print(f\"\\nCohen's d: {cohens_d:.3f}\")\n",
    "\n",
    "# Interpret\n",
    "if abs(cohens_d) < 0.2:\n",
    "    size = \"negligible\"\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    size = \"small\"\n",
    "elif abs(cohens_d) < 0.8:\n",
    "    size = \"medium\"\n",
    "else:\n",
    "    size = \"large\"\n",
    "    \n",
    "print(f\"Effect size interpretation: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 5: Interpret Results\n",
    "\n",
    "A researcher conducts two studies:\n",
    "\n",
    "**Study A**: n = 30, difference from expected = 0.8 points, p = 0.12  \n",
    "**Study B**: n = 5000, difference from expected = 0.08 points, p = 0.001\n",
    "\n",
    "Questions:\n",
    "1. Which study has a statistically significant result?\n",
    "2. Which study found a more practically meaningful effect?\n",
    "3. What explains this apparent paradox?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part6'></a>\n",
    "\n",
    "# Part 6: Relationships Between Variables\n",
    "\n",
    "Most research is about relationships: Does X predict Y? Are X and Y related?\n",
    "\n",
    "## Correlation\n",
    "\n",
    "**Correlation** measures the strength and direction of a linear relationship between two variables.\n",
    "\n",
    "üìù **Notation**: Pearson's correlation coefficient:\n",
    "\n",
    "$$r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2 \\sum_{i=1}^{n}(y_i - \\bar{y})^2}}$$\n",
    "\n",
    "This looks complicated! The intuition:\n",
    "- When X is above its mean, is Y also above its mean? (positive correlation)\n",
    "- When X is above its mean, is Y below its mean? (negative correlation)\n",
    "- No pattern? (no correlation)\n",
    "\n",
    "The result is always between -1 and +1:\n",
    "- r = +1: perfect positive correlation\n",
    "- r = 0: no linear correlation\n",
    "- r = -1: perfect negative correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different correlations\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Generate example data\n",
    "n = 100\n",
    "x = np.random.normal(0, 1, n)\n",
    "\n",
    "correlations = [0.9, 0.5, 0, -0.7]\n",
    "titles = ['Strong positive (r ‚âà 0.9)', 'Moderate positive (r ‚âà 0.5)', \n",
    "          'No correlation (r ‚âà 0)', 'Strong negative (r ‚âà -0.7)']\n",
    "\n",
    "for ax, r, title in zip(axes, correlations, titles):\n",
    "    # Generate y with desired correlation\n",
    "    noise = np.random.normal(0, 1, n)\n",
    "    y = r * x + np.sqrt(1 - r**2) * noise\n",
    "    \n",
    "    ax.scatter(x, y, alpha=0.6)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    actual_r = np.corrcoef(x, y)[0, 1]\n",
    "    ax.set_title(f'{title}\\n(actual r = {actual_r:.2f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations in our survey data\n",
    "variables = ['age', 'education_years', 'income', 'social_trust', 'political_engagement']\n",
    "correlation_matrix = survey[variables].corr()\n",
    "\n",
    "# Display as heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(correlation_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "\n",
    "# Add labels\n",
    "ax.set_xticks(range(len(variables)))\n",
    "ax.set_yticks(range(len(variables)))\n",
    "ax.set_xticklabels(variables, rotation=45, ha='right')\n",
    "ax.set_yticklabels(variables)\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(variables)):\n",
    "    for j in range(len(variables)):\n",
    "        text = ax.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                       ha='center', va='center', color='black', fontsize=10)\n",
    "\n",
    "ax.set_title('Correlation Matrix', fontsize=14)\n",
    "plt.colorbar(im, ax=ax, label='Correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: Looking at the correlation matrix, which pairs of variables are most strongly related? Does the direction (positive/negative) make intuitive sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r¬≤ : Variance Explained\n",
    "\n",
    "If you square the correlation, you get **r¬≤** (\"r-squared\"), which tells you the proportion of variance in Y that's explained by X.\n",
    "\n",
    "For example, if r = 0.6, then r¬≤ = 0.36, meaning 36% of the variance in Y can be explained by its linear relationship with X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education and income relationship\n",
    "r = survey['education_years'].corr(survey['income'])\n",
    "r_squared = r ** 2\n",
    "\n",
    "print(f\"Correlation between education and income: r = {r:.3f}\")\n",
    "print(f\"R-squared: {r_squared:.3f}\")\n",
    "print(f\"\\nInterpretation: {r_squared*100:.1f}% of the variance in income\")\n",
    "print(f\"can be explained by education years.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Correlation Does Not Imply Causation\n",
    "\n",
    "This is perhaps the most important warning in statistics. If X and Y are correlated, there are several possibilities:\n",
    "\n",
    "1. **X causes Y** (education ‚Üí income)\n",
    "2. **Y causes X** (reverse causation)\n",
    "3. **Z causes both X and Y** (confounding variable)\n",
    "4. **Pure coincidence** (spurious correlation)\n",
    "\n",
    "Correlation alone cannot distinguish between these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot with regression line\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.scatter(survey['education_years'], survey['income'], alpha=0.5)\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(survey['education_years'], survey['income'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(survey['education_years'].min(), survey['education_years'].max(), 100)\n",
    "ax.plot(x_line, p(x_line), 'r-', linewidth=2, label=f'r = {r:.2f}')\n",
    "\n",
    "ax.set_xlabel('Education (years)', fontsize=12)\n",
    "ax.set_ylabel('Income ($)', fontsize=12)\n",
    "ax.set_title('Education vs. Income', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 6: Explore Relationships\n",
    "\n",
    "1. Find the correlation between `social_trust` and `political_engagement`\n",
    "2. Is the correlation statistically significant?\n",
    "3. Create a scatter plot of the relationship\n",
    "4. Write one sentence interpreting what you found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# 1. Calculate correlation\n",
    "\n",
    "# 2. Test significance (hint: use stats.pearsonr)\n",
    "\n",
    "# 3. Create scatter plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='part7'></a>\n",
    "\n",
    "# Part 7: Bridge to ML/NLP ‚Äî Vectors and Matrices\n",
    "\n",
    "Everything we've covered so far is foundational for statistics. Now let's add one more perspective that's essential for machine learning and NLP: **thinking about data geometrically**.\n",
    "\n",
    "## The Key Insight\n",
    "\n",
    "> **Every row in your dataset is a point in space.**\n",
    "\n",
    "This might sound abstract, but it's the foundation of modern ML and NLP.\n",
    "\n",
    "## Vectors: Data as Points\n",
    "\n",
    "A **vector** is just an ordered list of numbers. In data science:\n",
    "\n",
    "- Each person/document/observation is a vector\n",
    "- Each number in the vector represents a feature/measurement\n",
    "\n",
    "üìù **Notation**:\n",
    "\n",
    "$$\\vec{v} = [v_1, v_2, ..., v_n]$$\n",
    "\n",
    "or in column form:\n",
    "\n",
    "$$\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}$$\n",
    "\n",
    "The number of elements is the **dimensionality** of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each row in our survey is a vector\n",
    "# Let's look at the first person represented as a vector\n",
    "\n",
    "person_0 = survey[['age', 'education_years', 'income', 'social_trust', 'political_engagement']].iloc[0].values\n",
    "\n",
    "print(\"First respondent as a vector:\")\n",
    "print(person_0)\n",
    "print(f\"\\nThis is a {len(person_0)}-dimensional vector.\")\n",
    "print(f\"\\nEach dimension represents:\")\n",
    "for i, col in enumerate(['age', 'education_years', 'income', 'social_trust', 'political_engagement']):\n",
    "    print(f\"  Dimension {i+1}: {col} = {person_0[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In 2D, we can visualize vectors as points\n",
    "# Let's use just age and income\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.scatter(survey['age'], survey['income'], alpha=0.6)\n",
    "\n",
    "# Highlight a few specific people as vectors\n",
    "for i in [0, 10, 50]:\n",
    "    ax.scatter(survey.iloc[i]['age'], survey.iloc[i]['income'], \n",
    "               s=100, edgecolor='red', facecolor='none', linewidth=2)\n",
    "    ax.annotate(f'Person {i}\\n({survey.iloc[i][\"age\"]:.0f}, ${survey.iloc[i][\"income\"]:,.0f})', \n",
    "                (survey.iloc[i]['age'], survey.iloc[i]['income']),\n",
    "                xytext=(10, 10), textcoords='offset points')\n",
    "\n",
    "ax.set_xlabel('Age (dimension 1)', fontsize=12)\n",
    "ax.set_ylabel('Income (dimension 2)', fontsize=12)\n",
    "ax.set_title('Each Person is a Point (Vector) in Space', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each dot is a person represented by a 2D vector: [age, income]\")\n",
    "print(\"With 5 features, each person is a point in 5-dimensional space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices: Collections of Vectors\n",
    "\n",
    "A **matrix** is a 2D array of numbers. In data science:\n",
    "\n",
    "- Each row is one observation (a vector)\n",
    "- Each column is one feature\n",
    "- The whole dataset is a matrix\n",
    "\n",
    "üìù **Notation**:\n",
    "\n",
    "$$X = \\begin{bmatrix} x_{1,1} & x_{1,2} & \\cdots & x_{1,p} \\\\ x_{2,1} & x_{2,2} & \\cdots & x_{2,p} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n,1} & x_{n,2} & \\cdots & x_{n,p} \\end{bmatrix}$$\n",
    "\n",
    "Where:\n",
    "- $n$ = number of rows (observations)\n",
    "- $p$ = number of columns (features)\n",
    "- $x_{i,j}$ = value in row $i$, column $j$\n",
    "\n",
    "We say $X$ has shape $(n \\times p)$ or \"n by p.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our survey as a matrix\n",
    "X = survey[['age', 'education_years', 'income', 'social_trust', 'political_engagement']].values\n",
    "\n",
    "print(f\"Matrix shape: {X.shape}\")\n",
    "print(f\"This means: {X.shape[0]} observations √ó {X.shape[1]} features\")\n",
    "print(f\"\\nFirst 5 rows of the matrix:\")\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Matters for NLP\n",
    "\n",
    "In NLP, we represent text as vectors:\n",
    "\n",
    "- **Bag of Words**: A document is a vector of word counts\n",
    "- **TF-IDF**: A document is a vector of weighted word frequencies\n",
    "- **Word Embeddings**: A word is a vector of learned features\n",
    "\n",
    "A corpus (collection of documents) becomes a matrix where:\n",
    "- Each row is a document\n",
    "- Each column is a word (or feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: documents as vectors\n",
    "# Vocabulary: ['cat', 'dog', 'fish', 'pet', 'animal']\n",
    "\n",
    "# Three \"documents\" represented as word count vectors\n",
    "doc1 = np.array([2, 0, 0, 1, 1])  # \"The cat is a pet animal. I love my cat.\"\n",
    "doc2 = np.array([0, 3, 0, 1, 1])  # \"My dog is the best pet. Dogs are great animals. Dog!\"\n",
    "doc3 = np.array([1, 1, 2, 1, 0])  # \"I have a cat, a dog, and two fish as pets.\"\n",
    "\n",
    "# Stack into a document-term matrix\n",
    "doc_term_matrix = np.vstack([doc1, doc2, doc3])\n",
    "\n",
    "print(\"Document-Term Matrix:\")\n",
    "print(\"Columns: ['cat', 'dog', 'fish', 'pet', 'animal']\")\n",
    "print(doc_term_matrix)\n",
    "print(f\"\\nShape: {doc_term_matrix.shape} (3 documents √ó 5 words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Similarity: The Dot Product\n",
    "\n",
    "Once we have vectors, we can measure how similar they are. The **dot product** is the foundation of similarity measurement.\n",
    "\n",
    "üìù **Notation**:\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i \\times b_i = a_1 b_1 + a_2 b_2 + ... + a_n b_n$$\n",
    "\n",
    "In plain English: **Multiply corresponding elements and add them up.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate dot product step by step\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# Step by step\n",
    "print(f\"Vector a: {a}\")\n",
    "print(f\"Vector b: {b}\")\n",
    "print(f\"\\nStep by step:\")\n",
    "print(f\"  {a[0]} √ó {b[0]} = {a[0] * b[0]}\")\n",
    "print(f\"  {a[1]} √ó {b[1]} = {a[1] * b[1]}\")\n",
    "print(f\"  {a[2]} √ó {b[2]} = {a[2] * b[2]}\")\n",
    "print(f\"  Sum: {a[0]*b[0]} + {a[1]*b[1]} + {a[2]*b[2]} = {np.dot(a, b)}\")\n",
    "\n",
    "# Using numpy\n",
    "print(f\"\\nNumPy dot product: {np.dot(a, b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "For comparing documents (or any vectors), we often use **cosine similarity**. It measures the angle between two vectors, ignoring their length.\n",
    "\n",
    "üìù **Notation**:\n",
    "\n",
    "$$\\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| \\times ||\\vec{b}||}$$\n",
    "\n",
    "Where $||\\vec{a}||$ is the length (magnitude) of vector $a$:\n",
    "\n",
    "$$||\\vec{a}|| = \\sqrt{\\sum_{i=1}^{n} a_i^2}$$\n",
    "\n",
    "Cosine similarity ranges from -1 to 1:\n",
    "- 1 = identical direction (very similar)\n",
    "- 0 = perpendicular (no similarity)\n",
    "- -1 = opposite direction\n",
    "\n",
    "**Why cosine similarity for documents?** It ignores document length. A long document about cats and a short document about cats will have high cosine similarity, even though their raw word counts differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    magnitude_a = np.sqrt(np.sum(a ** 2))\n",
    "    magnitude_b = np.sqrt(np.sum(b ** 2))\n",
    "    return dot_product / (magnitude_a * magnitude_b)\n",
    "\n",
    "# Compare our documents\n",
    "print(\"Document similarity (cosine):\")\n",
    "print(f\"Doc1 vs Doc2: {cosine_similarity(doc1, doc2):.3f}\")\n",
    "print(f\"Doc1 vs Doc3: {cosine_similarity(doc1, doc3):.3f}\")\n",
    "print(f\"Doc2 vs Doc3: {cosine_similarity(doc2, doc3):.3f}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Doc1 (cat-focused) and Doc2 (dog-focused) are least similar.\")\n",
    "print(\"Doc3 (mixed) is somewhat similar to both.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize in 2D (using just 'cat' and 'dog' dimensions)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot vectors as arrows from origin\n",
    "colors = ['blue', 'red', 'green']\n",
    "labels = ['Doc1 (cat)', 'Doc2 (dog)', 'Doc3 (mixed)']\n",
    "\n",
    "for i, (doc, color, label) in enumerate(zip([doc1, doc2, doc3], colors, labels)):\n",
    "    # Just use first two dimensions (cat, dog)\n",
    "    ax.arrow(0, 0, doc[0], doc[1], head_width=0.1, head_length=0.1, \n",
    "             fc=color, ec=color, linewidth=2)\n",
    "    ax.annotate(label, (doc[0], doc[1]), fontsize=12, \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "ax.set_xlim(-0.5, 3.5)\n",
    "ax.set_ylim(-0.5, 3.5)\n",
    "ax.set_xlabel('\"cat\" count', fontsize=12)\n",
    "ax.set_ylabel('\"dog\" count', fontsize=12)\n",
    "ax.set_title('Documents as Vectors\\n(showing only cat/dog dimensions)', fontsize=14)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Cosine similarity measures the ANGLE between vectors.\")\n",
    "print(\"Doc1 and Doc2 point in very different directions ‚Üí low similarity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication: Batch Operations\n",
    "\n",
    "In ML, we often need to perform many dot products at once. This is what **matrix multiplication** does.\n",
    "\n",
    "üìù **Notation**:\n",
    "\n",
    "If $A$ is $(m \\times n)$ and $B$ is $(n \\times p)$, then $C = AB$ is $(m \\times p)$.\n",
    "\n",
    "Each element $c_{i,j}$ is the dot product of row $i$ of $A$ with column $j$ of $B$.\n",
    "\n",
    "**In practice, you don't need to do this by hand** ‚Äî NumPy handles it. But understanding that it's \"many dot products at once\" helps you understand why linear algebra is so central to ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication example: computing all pairwise similarities at once\n",
    "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine\n",
    "\n",
    "# All pairwise cosine similarities\n",
    "similarity_matrix = sklearn_cosine(doc_term_matrix)\n",
    "\n",
    "print(\"Document Similarity Matrix:\")\n",
    "print(\"             Doc1    Doc2    Doc3\")\n",
    "for i, row in enumerate(similarity_matrix):\n",
    "    print(f\"Doc{i+1}        {row[0]:.3f}   {row[1]:.3f}   {row[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 7: Compute Document Similarity\n",
    "\n",
    "Here are three new documents represented as word count vectors:\n",
    "\n",
    "Vocabulary: `['data', 'science', 'machine', 'learning', 'statistics']`\n",
    "\n",
    "- Doc A: \"Data science uses statistics.\" ‚Üí `[1, 1, 0, 0, 1]`\n",
    "- Doc B: \"Machine learning is great.\" ‚Üí `[0, 0, 1, 1, 0]`  \n",
    "- Doc C: \"Data science and machine learning.\" ‚Üí `[1, 1, 1, 1, 0]`\n",
    "\n",
    "1. Calculate the cosine similarity between each pair of documents by hand\n",
    "2. Which two documents are most similar?\n",
    "3. Verify with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "doc_a = np.array([1, 1, 0, 0, 1])\n",
    "doc_b = np.array([0, 0, 1, 1, 0])\n",
    "doc_c = np.array([1, 1, 1, 1, 0])\n",
    "\n",
    "# Calculate similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary and Next Steps\n",
    "\n",
    "## What We Covered\n",
    "\n",
    "In this workshop, you learned:\n",
    "\n",
    "1. **Describing data**: Mean, variance, standard deviation ‚Äî summarizing what your data looks like\n",
    "\n",
    "2. **Distributions**: The normal distribution, z-scores, and why standardization matters\n",
    "\n",
    "3. **Sample to population**: The sampling distribution, Central Limit Theorem, and standard error ‚Äî understanding uncertainty\n",
    "\n",
    "4. **Confidence intervals**: Quantifying how precise our estimates are\n",
    "\n",
    "5. **Hypothesis testing**: P-values, statistical significance, and effect sizes ‚Äî evaluating evidence\n",
    "\n",
    "6. **Correlation**: Measuring relationships between variables (and its limitations)\n",
    "\n",
    "7. **Vectors and matrices**: Thinking geometrically about data, dot products, and cosine similarity\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Variability is the game**: Statistics is fundamentally about understanding and quantifying uncertainty\n",
    "- **Sample ‚Üí Population is the key inference**: We almost never see the whole population\n",
    "- **p < 0.05 is not magic**: Always consider effect sizes alongside significance\n",
    "- **Correlation ‚â† Causation**: One of the most important lessons in data analysis\n",
    "- **Data is geometry**: Every row is a point in space ‚Äî this perspective unlocks ML and NLP\n",
    "\n",
    "## Where This Leads\n",
    "\n",
    "| This Workshop | ML Workshop | NLP Workshop |\n",
    "|---------------|-------------|---------------|\n",
    "| Mean, variance, standardization | ‚Üí Preprocessing, feature scaling | |\n",
    "| Distributions, probability | ‚Üí Classification, probability outputs | |\n",
    "| Correlation, relationships | ‚Üí Feature selection, model coefficients | |\n",
    "| Vectors, matrices | ‚Üí | ‚Üí Bag of words, TF-IDF, embeddings |\n",
    "| Cosine similarity | ‚Üí | ‚Üí Document similarity, word vectors |\n",
    "\n",
    "## Recommended Next Steps\n",
    "\n",
    "1. **D-Lab's Python Machine Learning** workshop ‚Äî covers regression, classification, and preprocessing\n",
    "2. **D-Lab's Python Natural Language Processing** workshop ‚Äî covers text preprocessing, bag of words, and word embeddings\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Seeing Theory](https://seeing-theory.brown.edu/) ‚Äî Beautiful visualizations of statistical concepts\n",
    "- [StatQuest with Josh Starmer](https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw) ‚Äî Excellent YouTube explanations\n",
    "- [3Blue1Brown Linear Algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) ‚Äî Visual introduction to vectors and matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü•ä Take-Home Challenge: Full Analysis\n",
    "\n",
    "Using the survey data, conduct a mini-analysis:\n",
    "\n",
    "1. Choose two variables you think might be related\n",
    "2. Calculate descriptive statistics for both\n",
    "3. Test whether they're correlated (and if it's statistically significant)\n",
    "4. Calculate and interpret the effect size\n",
    "5. Create a visualization\n",
    "6. Write 2-3 sentences summarizing your findings\n",
    "\n",
    "Remember: Correlation doesn't mean causation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
